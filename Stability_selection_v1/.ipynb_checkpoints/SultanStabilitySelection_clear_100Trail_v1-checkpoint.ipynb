{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,ShuffleSplit\n",
    "from sklearn import svm\n",
    "import sys\n",
    "# sys.path.append('/home/ralfahad/PythonUtility/PTE')\n",
    "# from PhaseTE_MF import PhaseTE_MF\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from sklearn import svm, metrics,preprocessing\n",
    "#from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve, auc\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns; sns.set(font_scale=1.2)\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>label</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>1418</th>\n",
       "      <th>1419</th>\n",
       "      <th>1420</th>\n",
       "      <th>1421</th>\n",
       "      <th>1422</th>\n",
       "      <th>1423</th>\n",
       "      <th>1424</th>\n",
       "      <th>1425</th>\n",
       "      <th>1426</th>\n",
       "      <th>1427</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.534168e-11</td>\n",
       "      <td>6.231124e-10</td>\n",
       "      <td>-7.806000e-10</td>\n",
       "      <td>-8.063316e-10</td>\n",
       "      <td>4.260259e-10</td>\n",
       "      <td>8.792373e-11</td>\n",
       "      <td>-2.653489e-10</td>\n",
       "      <td>2.412203e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.956881e-10</td>\n",
       "      <td>2.334939e-10</td>\n",
       "      <td>-7.550422e-11</td>\n",
       "      <td>1.972939e-10</td>\n",
       "      <td>3.158191e-10</td>\n",
       "      <td>2.702473e-10</td>\n",
       "      <td>2.155772e-10</td>\n",
       "      <td>2.427227e-10</td>\n",
       "      <td>-1.328829e-10</td>\n",
       "      <td>-5.014970e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.256533e-10</td>\n",
       "      <td>1.718130e-10</td>\n",
       "      <td>1.905604e-11</td>\n",
       "      <td>1.276511e-11</td>\n",
       "      <td>-1.379325e-10</td>\n",
       "      <td>-6.269328e-11</td>\n",
       "      <td>-2.930309e-10</td>\n",
       "      <td>2.040993e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.343108e-10</td>\n",
       "      <td>2.950671e-11</td>\n",
       "      <td>-2.800265e-10</td>\n",
       "      <td>1.916018e-10</td>\n",
       "      <td>2.510715e-10</td>\n",
       "      <td>1.861655e-10</td>\n",
       "      <td>-2.830894e-10</td>\n",
       "      <td>2.069608e-10</td>\n",
       "      <td>-7.302684e-11</td>\n",
       "      <td>-2.662402e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 1430 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  label             0             1             2             3  \\\n",
       "0           0    0.0  7.534168e-11  6.231124e-10 -7.806000e-10 -8.063316e-10   \n",
       "1           1    0.0  2.256533e-10  1.718130e-10  1.905604e-11  1.276511e-11   \n",
       "\n",
       "              4             5             6             7      ...       \\\n",
       "0  4.260259e-10  8.792373e-11 -2.653489e-10  2.412203e-10      ...        \n",
       "1 -1.379325e-10 -6.269328e-11 -2.930309e-10  2.040993e-10      ...        \n",
       "\n",
       "           1418          1419          1420          1421          1422  \\\n",
       "0  1.956881e-10  2.334939e-10 -7.550422e-11  1.972939e-10  3.158191e-10   \n",
       "1  2.343108e-10  2.950671e-11 -2.800265e-10  1.916018e-10  2.510715e-10   \n",
       "\n",
       "           1423          1424          1425          1426          1427  \n",
       "0  2.702473e-10  2.155772e-10  2.427227e-10 -1.328829e-10 -5.014970e-10  \n",
       "1  1.861655e-10 -2.830894e-10  2.069608e-10 -7.302684e-11 -2.662402e-10  \n",
       "\n",
       "[2 rows x 1430 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the metadata\n",
    "# path='/home/sultan/EEG/Source_Level_Analysis/25sam_10ms_clear_all_erp.csv'\n",
    "# path=\"/home/sultan/EEG/Source_Level_Analysis/50tr10ms_all_clear_erp.csv\"\n",
    "# path=\"/home/sultan/EEG/Source_Level_Analysis/75sam_10ms_clear_all_erp.csv\"\n",
    "path=\"/home/sultan/EEG/Source_Level_Analysis/100sam_10ms_clear_all_erp.csv\"\n",
    "Metadata=pd.read_csv(path)\n",
    "Metadata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1863, 1428), (1863,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=Metadata.iloc[:,2:]\n",
    "y=Metadata['label']\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from scipy import linalg as LA\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3, linewidth=110, suppress=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def LDA(data, labels, dim_rescale):\n",
    "    '''\n",
    "    Linear Discriminant Analysis\n",
    "    pass in:\n",
    "        (i) a raw data array--features encoded in the cols;\n",
    "            one data instance per row;\n",
    "        (ii) EV, explanatory variable, is included in D as last column;\n",
    "        (iii) the LDA flag is set to False so PCA is the default techique;\n",
    "            if both LDA & EV are set to True then LDA is performed\n",
    "            instead of PCA\n",
    "    returns:\n",
    "        (i) eigenvalues (1D array);\n",
    "        (ii) eigenvectors (2D array)\n",
    "        (iii) covariance matrix\n",
    "    some numerical assertions:\n",
    "    >>> # sum of the eigenvalues is equal to trace of R\n",
    "    >>> x = R.trace()\n",
    "    >>> x1 = eva.sum()\n",
    "    >>> NP.allclose(x, x1)\n",
    "    True\n",
    "    >>> # determinant of R is product of eigenvalues\n",
    "    >>> q = LA.det(R)\n",
    "    >>> q1 = NP.prod(eva)\n",
    "    >>> NP.allclose(q, q1)\n",
    "    True\n",
    "    '''\n",
    "    assert data.shape[0] == labels.shape[0]\n",
    "    # mean center the data array\n",
    "    data -= data.mean(axis=0)\n",
    "    nrow, ndim = data.shape\n",
    "    # pre-allocate sw, sb arrays (both same shape as covariance matrix)\n",
    "    # s_wc: array encoding 'within class' scatter\n",
    "    # s_bc: array encoding 'between class' scatter\n",
    "    s_wc = np.zeros((ndim, ndim))\n",
    "    s_bc = np.zeros((ndim, ndim))\n",
    "    R = np.cov(data.T)\n",
    "    classes = np.unique(labels)\n",
    "    for c in range(len(classes)):\n",
    "        # create an index only for data rows whose class label = classes[c]\n",
    "        idx = np.squeeze(np.where(labels == classes[c]))\n",
    "        d = np.squeeze(data[idx,:])\n",
    "        class_cov = np.cov(d.T)\n",
    "        s_wc += float(idx.shape[0]) / nrow * class_cov\n",
    "    s_bc = R - s_wc\n",
    "    # now solve for w then compute the mapped data\n",
    "    evals, evecs = LA.eig(s_wc, s_bc)\n",
    "    np.ascontiguousarray(evals)\n",
    "    np.ascontiguousarray(evecs)\n",
    "    # sort the eigenvectors based on eigenvalues sort order\n",
    "    idx = np.argsort(evals)\n",
    "    idx = idx[::-1]\n",
    "    evecs = evecs[:,idx]\n",
    "    # take just number of eigenvectors = dim_rescale\n",
    "    evecs_dr = evecs[:,:dim_rescale]\n",
    "    # multiply data array & remaining set of eigenvectors\n",
    "    rescaled_data = np.dot(data, evecs_dr)\n",
    "    return rescaled_data, evecs_dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescaled_data, w = LDA(X_WM_Row_scale, y_WM_Row_scale,50)\n",
    "a, w1 = LDA(np.asarray(X), np.asarray(y), 50)# X_WM_Row_scale,y_WM_Row_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=3, init='pca')\n",
    "Y = tsne.fit_transform(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lb2=\"Fast RT\"\n",
    "#lb3=\"Medium RT\"\n",
    "lb1=\"NH\"\n",
    "lb2=\"HI\"\n",
    "fileName=\"tSNE_KDE_plot_edit_2\"\n",
    "\n",
    "a1=Y[np.where(y==0), 0]\n",
    "b1=Y[np.where(y==0), 1]\n",
    "c1= Y[np.where(y==1), 0]\n",
    "d1 = Y[np.where(y==1), 1]\n",
    "# e1= Y[np.where(y==2), 0]\n",
    "# f1 = Y[np.where(y==2), 1]\n",
    "\n",
    "dftt1=pd.concat([pd.DataFrame(a1),pd.DataFrame(b1)], axis=0).transpose()\n",
    "dftt2=pd.concat([pd.DataFrame(c1),\n",
    "                pd.DataFrame(d1)], axis=0).transpose()\n",
    "# dftt3=pd.concat([pd.DataFrame(e1),\n",
    "#                 pd.DataFrame(f1)], axis=0).transpose()\n",
    "print dftt1.shape,dftt2.shape#,dftt3.shape\n",
    "dftt1.columns= ['a','b']\n",
    "dftt2.columns= ['c','d']\n",
    "#dftt3.columns= ['e','f']\n",
    "ax = sns.kdeplot(dftt1['a'],dftt1['b'], cmap=\"Greens\", shade=False, shade_lowest=False)\n",
    "ax = sns.regplot(dftt1['a'], dftt1['b'],marker= '.', color='g',  fit_reg=False,label=lb1)\n",
    "ax.legend(loc=\"best\",framealpha=0.0)\n",
    "# #plt.legend(loc='upper left')\n",
    "ax = sns.kdeplot(dftt2['c'],dftt2['d'],cmap=\"Reds\", shade=False, shade_lowest=False)\n",
    "ax = sns.regplot(dftt2['c'], dftt2['d'],marker= '+', color='r',  fit_reg=False, label=lb2)\n",
    "\n",
    "# ax = sns.kdeplot(dftt3['e'],dftt3['f'],cmap=\"Blues\", shade=False, shade_lowest=False)\n",
    "# ax = sns.regplot(dftt3['e'], dftt3['f'],marker= '*', color='b',  fit_reg=False, label=lb3)\n",
    "ax.legend(loc=\"best\",framealpha=0.0)\n",
    "\n",
    "# Add labels to the plot\n",
    "red = sns.color_palette(\"Greens\")[-2]\n",
    "blue = sns.color_palette(\"Reds\")[-2]\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('')\n",
    "ax.yaxis.set_ticklabels([])\n",
    "ax.xaxis.set_ticklabels([])\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "# ax.spines['left'].set_visible(False)\n",
    "\n",
    "#sns.despine()\n",
    "# Save image\n",
    "\n",
    "save_format='png'\n",
    "print str(fileName)+'.'+save_format\n",
    "plt.savefig(fileName+'.'+save_format,dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Micro AUC for multiclass classification \n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from scipy import interp\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# def MulticlassAuc(y_test,y_p): # y_test, y_p is the test and predict data\n",
    "#     lb = preprocessing.LabelBinarizer() # binarize the data\n",
    "#     lb.fit(y_test)\n",
    "#     n_classes=len(lb.classes_)\n",
    "#     y_test_b=lb.transform(y_test)\n",
    "#     y_p_b=lb.transform(y_p)\n",
    "\n",
    "#     # Compute ROC curve and ROC area for each class\n",
    "#     fpr = dict()\n",
    "#     tpr = dict()\n",
    "#     roc_auc = dict()\n",
    "#     for i in range(3):\n",
    "#         fpr[i], tpr[i], _ = roc_curve(y_test_b[:, i], y_p_b[:, i])\n",
    "#         roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "#     # First aggregate all false positive rates\n",
    "#     all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "#     # Then interpolate all ROC curves at this points\n",
    "#     mean_tpr = np.zeros_like(all_fpr)\n",
    "#     for i in range(n_classes):\n",
    "#         mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "#     # Finally average it and compute AUC\n",
    "#     mean_tpr /= n_classes\n",
    "\n",
    "#     fpr[\"macro\"] = all_fpr\n",
    "#     tpr[\"macro\"] = mean_tpr\n",
    "#     roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "#     return roc_auc[\"macro\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply SVM on Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_range = np.logspace(-2, 2, 5)\n",
    "gamma_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X=preprocessing.scale(X)\n",
    "print X.shape,y.shape\n",
    "from sklearn.preprocessing import label_binarize\n",
    "y=label_binarize(np.asarray(y),[0,1])\n",
    "\n",
    "#C_range = np.logspace(-2, 10, 13)\n",
    "# gamma_range = np.logspace(-2, 2, 5)\n",
    "gamma_range = [0.01,0.002,0.00069,0.0007,0.0005]\n",
    "C_range = np.logspace(-2, 2, 5)\n",
    "#gamma_range = np.logspace(-2, 2, 5)\n",
    "\n",
    "print C_range,gamma_range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classifiaction:\n",
    "# #Splitting\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.4,\n",
    "                                                    random_state=42)\n",
    "cv = ShuffleSplit(X_train.shape[0], test_size=0.4, random_state=42)\n",
    "\n",
    "# Define Classifier\n",
    "svr = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Deffine tuning parameter\n",
    "C_range = np.logspace(-2, 2, 5)\n",
    "# gamma_range = np.logspace(-2, 2, 5)\n",
    "gamma_range = [0.01,0.002,0.00069,0.0007,0.0005]\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "clf_Tune = GridSearchCV(estimator=svr, cv=5, param_grid=param_grid,n_jobs=-1, verbose=True)\n",
    "\n",
    "clf_Tune.fit(X_train,y_train)\n",
    "print 'Finish tuning'    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Micro AUC for multiclass classification \n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def MulticlassAuc(y_test,y_p): # y_test, y_p is the test and predict data\n",
    "    lb = preprocessing.LabelBinarizer() # binarize the data\n",
    "    lb.fit(y_test)\n",
    "    n_classes=len(lb.classes_)\n",
    "    y_test_b=lb.transform(y_test)\n",
    "    y_p_b=lb.transform(y_p)\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(3):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_test_b[:, i], y_p_b[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = all_fpr\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "    return roc_auc[\"macro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_p = clf_Tune.best_estimator_.predict(X_test)\n",
    "ACC=classification_report(y_test, y_p)\n",
    "print ACC\n",
    "ACC_AVG=accuracy_score(y_test, y_p)\n",
    "print ACC_AVG\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test,y_p)\n",
    "AUC_Th2_T=metrics.auc(fpr, tpr)\n",
    "print AUC_Th2_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to move the midpoint of a colormap to be around\n",
    "# the values of interest.\n",
    "from matplotlib.colors import Normalize\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "    \n",
    "scores = clf_Tune.cv_results_['mean_test_score'].reshape(len(C_range),len(gamma_range))\n",
    "#print scores.shape,len(C_range),len(gamma_range)\n",
    "plt.figure(figsize=(8, 6))\n",
    "#plt.subplots_adjust(left=.2, right=0.95, bottom=0.15, top=0.95)\n",
    "plt.imshow(scores, interpolation='nearest', cmap=plt.cm.hot, norm=MidpointNormalize(vmin=0.2, midpoint=0.92))\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('C')\n",
    "plt.colorbar()\n",
    "plt.xticks(np.arange(len(gamma_range)), gamma_range, rotation=45)\n",
    "plt.yticks(np.arange(len(C_range)), C_range)\n",
    "plt.title('Validation accuracy={}, best {}' .format(clf_Tune.best_score_,clf_Tune.best_params_))\n",
    "\n",
    "#filename='ParameterTuning'\n",
    "#save_format='png'\n",
    "#print filename+'.'+save_format\n",
    "#pp='home/ralfahad/Pictures'\n",
    "#plt.savefig(filename+'.'+save_format,dpi=100)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Significant correlation with stability selections "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X=preprocessing.scale(X)\n",
    "print X.shape,y.shape\n",
    "from sklearn.preprocessing import label_binarize\n",
    "y=label_binarize(np.asarray(y),[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE Visulaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "\n",
    "# from scipy import linalg as LA\n",
    "\n",
    "\n",
    "# np.set_printoptions(precision=3, linewidth=110, suppress=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def LDA(data, labels, dim_rescale):\n",
    "#     '''\n",
    "#     Linear Discriminant Analysis\n",
    "#     pass in:\n",
    "#         (i) a raw data array--features encoded in the cols;\n",
    "#             one data instance per row;\n",
    "#         (ii) EV, explanatory variable, is included in D as last column;\n",
    "#         (iii) the LDA flag is set to False so PCA is the default techique;\n",
    "#             if both LDA & EV are set to True then LDA is performed\n",
    "#             instead of PCA\n",
    "#     returns:\n",
    "#         (i) eigenvalues (1D array);\n",
    "#         (ii) eigenvectors (2D array)\n",
    "#         (iii) covariance matrix\n",
    "#     some numerical assertions:\n",
    "#     >>> # sum of the eigenvalues is equal to trace of R\n",
    "#     >>> x = R.trace()\n",
    "#     >>> x1 = eva.sum()\n",
    "#     >>> NP.allclose(x, x1)\n",
    "#     True\n",
    "#     >>> # determinant of R is product of eigenvalues\n",
    "#     >>> q = LA.det(R)\n",
    "#     >>> q1 = NP.prod(eva)\n",
    "#     >>> NP.allclose(q, q1)\n",
    "#     True\n",
    "#     '''\n",
    "#     assert data.shape[0] == labels.shape[0]\n",
    "#     # mean center the data array\n",
    "#     data -= data.mean(axis=0)\n",
    "#     nrow, ndim = data.shape\n",
    "#     # pre-allocate sw, sb arrays (both same shape as covariance matrix)\n",
    "#     # s_wc: array encoding 'within class' scatter\n",
    "#     # s_bc: array encoding 'between class' scatter\n",
    "#     s_wc = np.zeros((ndim, ndim))\n",
    "#     s_bc = np.zeros((ndim, ndim))\n",
    "#     R = np.cov(data.T)\n",
    "#     classes = np.unique(labels)\n",
    "#     for c in range(len(classes)):\n",
    "#         # create an index only for data rows whose class label = classes[c]\n",
    "#         idx = np.squeeze(np.where(labels == classes[c]))\n",
    "#         d = np.squeeze(data[idx,:])\n",
    "#         class_cov = np.cov(d.T)\n",
    "#         s_wc += float(idx.shape[0]) / nrow * class_cov\n",
    "#     s_bc = R - s_wc\n",
    "#     # now solve for w then compute the mapped data\n",
    "#     evals, evecs = LA.eig(s_wc, s_bc)\n",
    "#     np.ascontiguousarray(evals)\n",
    "#     np.ascontiguousarray(evecs)\n",
    "#     # sort the eigenvectors based on eigenvalues sort order\n",
    "#     idx = np.argsort(evals)\n",
    "#     idx = idx[::-1]\n",
    "#     evecs = evecs[:,idx]\n",
    "#     # take just number of eigenvectors = dim_rescale\n",
    "#     evecs_dr = evecs[:,:dim_rescale]\n",
    "#     # multiply data array & remaining set of eigenvectors\n",
    "#     rescaled_data = np.dot(data, evecs_dr)\n",
    "#     return rescaled_data, evecs_dr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Micro AUC for multiclass classification \n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from scipy import interp\n",
    "# from sklearn import preprocessing\n",
    "\n",
    "# def MulticlassAuc(y_test,y_p): # y_test, y_p is the test and predict data\n",
    "#     lb = preprocessing.LabelBinarizer() # binarize the data\n",
    "#     lb.fit(y_test)\n",
    "#     n_classes=len(lb.classes_)\n",
    "#     y_test_b=lb.transform(y_test)\n",
    "#     y_p_b=lb.transform(y_p)\n",
    "\n",
    "#     # Compute ROC curve and ROC area for each class\n",
    "#     fpr = dict()\n",
    "#     tpr = dict()\n",
    "#     roc_auc = dict()\n",
    "#     for i in range(3):\n",
    "#         fpr[i], tpr[i], _ = roc_curve(y_test_b[:, i], y_p_b[:, i])\n",
    "#         roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "#     # First aggregate all false positive rates\n",
    "#     all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "#     # Then interpolate all ROC curves at this points\n",
    "#     mean_tpr = np.zeros_like(all_fpr)\n",
    "#     for i in range(n_classes):\n",
    "#         mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "#     # Finally average it and compute AUC\n",
    "#     mean_tpr /= n_classes\n",
    "\n",
    "#     fpr[\"macro\"] = all_fpr\n",
    "#     tpr[\"macro\"] = mean_tpr\n",
    "#     roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "#     return roc_auc[\"macro\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (RandomizedLasso, lasso_stability_path,\n",
    "                                  LassoLarsCV)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "#Model Library\n",
    "from sklearn.linear_model import (RandomizedLasso, lasso_stability_path, LassoLarsCV)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, RandomizedLogisticRegression\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "# Performance analysis library \n",
    "from sklearn.model_selection import KFold, cross_val_score, LeaveOneOut, cross_val_predict\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import train_test_split # test train split\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    #warnings.simplefilter('ignore', UserWarning)\n",
    "    warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "    lars_cv = LassoLarsCV(cv=10).fit(X, y)\n",
    "lars_cv.alphas_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the RandomizedLasso: we use a paths going down to .1*alpha_max\n",
    "# to avoid exploring the regime in which very noisy variables enter\n",
    "# the model\n",
    "alphas = np.linspace(lars_cv.alphas_[0], .1 * lars_cv.alphas_[0], 10)\n",
    "print alphas\n",
    "clf = RandomizedLasso(alpha=alphas, random_state=42,max_iter=1000).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "data=clf.scores_\n",
    "fig, ax = plt.subplots()\n",
    "counts, bins, patches = ax.hist(data,10 ,edgecolor='gray')\n",
    "\n",
    "# Set the ticks to be at the edges of the bins.\n",
    "ax.set_xticks(bins)\n",
    "\n",
    "# Set the xaxis's tick labels to be formatted with 1 decimal place...\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%0.1f'))\n",
    "\n",
    "\n",
    "# Label the raw counts and the percentages below the x-axis...\n",
    "bin_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
    "for count, x in zip(counts, bin_centers):\n",
    "    # Label the raw counts\n",
    "    ax.annotate(str(count), xy=(x, 0), xycoords=('data', 'axes fraction'),\n",
    "        xytext=(0, -18), textcoords='offset points', va='top', ha='center')\n",
    "\n",
    "    # Label the percentages\n",
    "    percent = '%0.0f%%' % (100 * float(count) / counts.sum())\n",
    "    ax.annotate(percent, xy=(x, 0), xycoords=('data', 'axes fraction'),\n",
    "        xytext=(0, -32), textcoords='offset points', va='top', ha='center')\n",
    "\n",
    "\n",
    "# Give ourselves some more room at the bottom of the plot\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Th_2_Bins_Index= np.where((bins<=0.9) & (bins>=0.1)) # take the bins within a range\n",
    "print bins\n",
    "Th_2_Bins=bins[Th_2_Bins_Index]\n",
    "print Th_2_Bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X=preprocessing.scale(X)\n",
    "print X.shape,y.shape\n",
    "from sklearn.preprocessing import label_binarize\n",
    "y=label_binarize(np.asarray(y),[0,1])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "print X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:,1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train)#[:,np.squeeze(np.asarray(np.where(clf.scores_>=0.16)))].shape#,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 2, 5)\n",
    "gamma_range = [0.01,0.002,0.00069,0.0007,0.0005]\n",
    "# gamma_range = np.logspace(-2, 2, 5)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "#clf_Tune = GridSearchCV(estimator=svr, cv=1, param_grid=param_grid,n_jobs=-1)\n",
    "clf_Tune = GridSearchCV(estimator=svr, cv=5, param_grid=param_grid,n_jobs=-1, verbose=True)\n",
    "clf_Tune.fit(X_train[:,np.squeeze(np.asarray(np.where(clf.scores_>=0.16)))],y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tuned parameter on to get model\n",
    "y_p = clf_Tune.best_estimator_.predict(X_test[:,np.squeeze(np.asarray(np.where(clf.scores_>=0.16)))])\n",
    "\n",
    "print accuracy_score(y_test, y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "#cv = ShuffleSplit(X_train.shape[0], test_size=0.2, random_state=42)\n",
    "\n",
    "# Define Classifier\n",
    "svr = svm.SVC(kernel='rbf')\n",
    "\n",
    "# Deffine tuning parameter\n",
    "# C_range = np.logspace(-2, 10, 13)\n",
    "# gamma_range = np.logspace(-9, 3, 13)\n",
    "\n",
    "C_range = np.logspace(-2, 2, 5)\n",
    "gamma_range = [0.01,0.002,0.00069,0.0007,0.0005]\n",
    "#gamma_range = np.logspace(-2, 2, 5)\n",
    "param_grid = dict(gamma=gamma_range, C=C_range)\n",
    "#clf_Tune = GridSearchCV(estimator=svr, cv=1, param_grid=param_grid,n_jobs=-1)\n",
    "clf_Tune = GridSearchCV(estimator=svr, cv=5, param_grid=param_grid,n_jobs=-1, verbose=True)\n",
    "##  Define LeaveOneOutCrossValidation\n",
    "#loocv = LeaveOneOut()\n",
    "ACC_Th2=[]\n",
    "AUC_Th2=[]\n",
    "Bins=[]\n",
    "for i in Th_2_Bins:\n",
    "    print i\n",
    "    try:\n",
    "        print X_train[:,np.squeeze(np.asarray(np.where(clf.scores_>=i)))].shape\n",
    "        \n",
    "        #Hyper parameter Tuning \n",
    "        clf_Tune.fit(X_train[:,np.squeeze(np.asarray(np.where(clf.scores_>=i)))],y_train)\n",
    "        print X_train[:,np.squeeze(np.asarray(np.where(clf.scores_>=i)))].shape ,y_train.shape\n",
    "        print 'Finish tuning'\n",
    "\n",
    "        # use tuned parameter on to get model\n",
    "        y_p = clf_Tune.best_estimator_.predict(X_test[:,np.squeeze(np.asarray(np.where(clf.scores_>=i)))])\n",
    "\n",
    "        ACC_Th2_T=accuracy_score(y_test, y_p)\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(y_test,y_p)\n",
    "        AUC_Th2_T=metrics.auc(fpr, tpr)\n",
    "        print 'Shape', X_test[:,np.squeeze(np.asarray(np.where(clf.scores_>=i)))].shape\n",
    "        print 'ACC',ACC_Th2_T,AUC_Th2_T\n",
    "        ACC_Th2.append(ACC_Th2_T)\n",
    "        AUC_Th2.append(AUC_Th2_T)\n",
    "        Bins.append(i)\n",
    "    except:\n",
    "        print 'error at:',i\n",
    "    # false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_p)\n",
    "    # roc_auc_T = auc(false_positive_rate, true_positive_rate)\n",
    "    # print 'AUC',roc_auc_T\n",
    "    # AUC_Th2.append(roc_auc_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(y_test, y_p)\n",
    "# MulticlassAuc(y_test,y_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "#         ACC_Th2_T=accuracy_score(y_test, y_p)\n",
    "#         AUC_Th2_T=MulticlassAuc(y_test,y_p)\n",
    "#         print 'Shape', X_test[:,np.squeeze(np.asarray(np.where(clf.scores_>=i)))].shape\n",
    "#         print 'ACC',ACC_Th2_T,AUC_Th2_T\n",
    "#         ACC_Th2.append(ACC_Th2_T)\n",
    "#         AUC_Th2.append(AUC_Th2_T)\n",
    "#         Bins.append(i)\n",
    "#     except:\n",
    "#         print 'error at:',i\n",
    "#     # false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_p)\n",
    "#     # roc_auc_T = auc(false_positive_rate, true_positive_rate)\n",
    "#     # print 'AUC',roc_auc_T\n",
    "#     # AUC_Th2.append(roc_auc_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_Th2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "Th_2_Bins=np.asarray(Bins)\n",
    "data=clf.scores_\n",
    "fig, ax = plt.subplots()\n",
    "fig.subplots_adjust(right=0.75)\n",
    "#counts, bins, patches = ax.hist(data,10 ,edgecolor='gray')\n",
    "counts, bins, patches = ax.hist(data,10,facecolor=\"None\",edgecolor='blue', lw=1)\n",
    "# Set the ticks to be at the edges of the bins.\n",
    "ax.set_xticks(bins)\n",
    "# Set the xaxis's tick labels to be formatted with 1 decimal place...\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%0.1f'))\n",
    "\n",
    "\n",
    "bin_centers = 0.5 * np.diff(bins) + bins[:-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#show % value\n",
    "for count, x in zip(counts, bin_centers):\n",
    "    # Label the raw counts\n",
    "    ax.annotate(int(count), xy=(x, 0), xycoords=('data', 'axes fraction'),\n",
    "        xytext=(0, -18), textcoords='offset points', va='top', ha='center')\n",
    "\n",
    "    # Label the percentages\n",
    "    percent = '%0.0f%%' % (100 * float(count) / counts.sum())\n",
    "    ax.annotate(percent, xy=(x, 0), xycoords=('data', 'axes fraction'),\n",
    "        xytext=(0, -32), textcoords='offset points', va='top', ha='center')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Give ourselves some more room at the bottom of the plot\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(Th_2_Bins,AUC_Th2, color='red',ls='dashed' )#,THbin,CCC_Per)\n",
    "ax2.set_frame_on(False)\n",
    "ax2.set_ylabel('AUC', color='red')\n",
    "ax2.patch.set_visible(False)\n",
    "for i,j in zip(Th_2_Bins,AUC_Th2):\n",
    "    #ax2.annotate((\"%.2f\" % j),xy=(i+0.04,j-0.01), color ='red') ##############################\n",
    "    ax2.annotate((\"%.2f\" % j),xy=(i,j-0.01), color ='red') ##############################\n",
    "    ax2.plot(i,j, marker='o', markersize=7, color=\"red\")\n",
    "\n",
    "\n",
    "    \n",
    "ax3 = ax.twinx()\n",
    "ax3.plot(Th_2_Bins,ACC_Th2, color='black')#,THbin,CCC_Per)\n",
    "ax3.set_frame_on(False)\n",
    "ax3.set_ylabel('Accuracy', color='black')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i,j in zip(Th_2_Bins,ACC_Th2):\n",
    "     #ax3.annotate((\"%0.2f\" % j),xy=(i-0.05,j), color='black')\n",
    "    #ax3.annotate((\"%0.2f\" % j),xy=(i-0.1,j), color='black')##################################\n",
    "    ax3.annotate((\"%0.2f\" % j),xy=(i,j+0.01), color='black')##################################\n",
    "    ax3.plot(i,j, marker='*', markersize=10, color=\"black\")\n",
    "    #ax3.annotate((\"%0.2f,%0.2f\" % (i,j)),xy=(i-0.05,j), color='green')\n",
    "    #ax3.annotate((\"%0.3f\" %j),xy=(i,j), color='green')\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Move the last y-axis spine over to the right by 20% of the width of the axes\n",
    "ax3.spines['right'].set_position(('axes', 1.15))\n",
    "ax3.spines['right'].set_visible(True)\n",
    "\n",
    "# To make the border of the right-most axis visible, we need to turn the frame\n",
    "# on. This hides the other plots, however, so we need to turn its fill off.\n",
    "ax3.set_frame_on(True)\n",
    "ax3.patch.set_visible(False)\n",
    "\n",
    "\n",
    "#ax.plot(bins,np.linspace(0,1,11))\n",
    "plt.subplots_adjust(bottom=0.15)\n",
    "plt.grid(False,which='both')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.yaxis.label.set_color('Blue')\n",
    "plt.tight_layout()\n",
    "# #Save the image\n",
    "\n",
    "filename='Corr_StabilitySelection_TH2'\n",
    "save_format='png'\n",
    "#print filename+'.'+save_format\n",
    "#pp='home/ralfahad/Pictures'\n",
    "#plt.savefig(filename+'.'+save_format,dpi=100)\n",
    "plt.savefig(filename+'.'+save_format,dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in zip(Th_2_Bins,AUC_Th2):\n",
    "#     print i,j\n",
    "All_ACC=[]\n",
    "All_AUC=[]\n",
    "ALL_Bins=[]\n",
    "NumberofElement=[]\n",
    "for i,j,k in zip(Th_2_Bins,ACC_Th2,AUC_Th2):\n",
    "    Th2Index=np.squeeze(np.asarray(np.where(clf.scores_>=i)))\n",
    "    print (\"{0:.2f}\".format(i)),(\"{0:.2f}\".format(j)),(\"{0:.2f}\".format(k)),len(Th2Index)\n",
    "    NumberofElement.append(len(Th2Index))\n",
    "    ALL_Bins.append(\"{0:.2f}\".format(i))\n",
    "    All_ACC.append(\"{0:.2f}\".format(j))\n",
    "    All_AUC.append(\"{0:.2f}\".format(k))\n",
    "\n",
    "Selected_Feature_Result=pd.concat([pd.DataFrame(ALL_Bins),pd.DataFrame(All_ACC),\n",
    "           pd.DataFrame(All_AUC),pd.DataFrame(NumberofElement)],axis=1)\n",
    "Selected_Feature_Result.columns=['Threshold','ACC','AUC','NoEle']\n",
    "Selected_Feature_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to chage gamma range . It is selecting too many features\n",
    "\n",
    "# Let 0.34 is our best Thr. TO get the index numer with this thr\n",
    "\n",
    "\n",
    "fealoc=np.squeeze(np.asarray(np.where(clf.scores_>=0.34)))\n",
    "fealoc\n",
    "\n",
    "# Do you get it? yes > How about number of support of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI=fealoc%68\n",
    "ROI\n",
    "np.unique(ROI,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(ROI))\n",
    "# np.unique(ROI,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for th in [0.17, 0.25,0.34,0.42]:\n",
    "    clf_Tune.fit(X_train[:,np.squeeze(np.asarray(np.where(clf.scores_>=th)))],y_train)\n",
    "    print X_train[:,np.squeeze(np.asarray(np.where(clf.scores_>=th)))].shape ,y_train.shape\n",
    "    print 'Finish tuning'\n",
    "    print float(len(clf_Tune.best_estimator_.support_vectors_))/X_train[:,np.squeeze(np.asarray(np.where(clf.scores_>=th)))].shape[0]*100\n",
    "# use tuned parameter on to get model\n",
    "#y_p = clf_Tune.best_estimator_.predict(X_test[:,np.squeeze(np.asarray(np.where(clf.scores_>=i)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(len(clf_Tune.best_estimator_.support_vectors_))/X_train[:,np.squeeze(np.asarray(np.where(clf.scores_>=0.25)))].shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result it overfitting need to be less or equal to 40%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sultan----------->>>>>>>>>>>>>>>>>>>> End Here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[0,1,7,8,68,1427]\n",
    "b=np.array(a)\n",
    "b%68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
