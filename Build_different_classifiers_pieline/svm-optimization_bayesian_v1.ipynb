{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization of SVM [link](https://thuijskens.github.io/2016/12/29/bayesian-optimisation/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "# Load the Python scripts that contain the Bayesian optimization code\n",
    "#%run ./../python/gp.py\n",
    "#%run ./../python/plotters.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.gaussian_process as gp\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def expected_improvement(x, gaussian_process, evaluated_loss, greater_is_better=False, n_params=1):\n",
    "    \"\"\" expected_improvement\n",
    "\n",
    "    Expected improvement acquisition function.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        x: array-like, shape = [n_samples, n_hyperparams]\n",
    "            The point for which the expected improvement needs to be computed.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: Numpy array.\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        n_params: int.\n",
    "            Dimension of the hyperparameter space.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    x_to_predict = x.reshape(-1, n_params)\n",
    "\n",
    "    mu, sigma = gaussian_process.predict(x_to_predict, return_std=True)\n",
    "\n",
    "    if greater_is_better:\n",
    "        loss_optimum = np.max(evaluated_loss)\n",
    "    else:\n",
    "        loss_optimum = np.min(evaluated_loss)\n",
    "\n",
    "    scaling_factor = (-1) ** (not greater_is_better)\n",
    "\n",
    "    # In case sigma equals zero\n",
    "    with np.errstate(divide='ignore'):\n",
    "        Z = scaling_factor * (mu - loss_optimum) / sigma\n",
    "        expected_improvement = scaling_factor * (mu - loss_optimum) * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        expected_improvement[sigma == 0.0] == 0.0\n",
    "\n",
    "    return -1 * expected_improvement\n",
    "\n",
    "\n",
    "def sample_next_hyperparameter(acquisition_func, gaussian_process, evaluated_loss, greater_is_better=False,\n",
    "                               bounds=(0, 10), n_restarts=25):\n",
    "    \"\"\" sample_next_hyperparameter\n",
    "\n",
    "    Proposes the next hyperparameter to sample the loss function for.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        acquisition_func: function.\n",
    "            Acquisition function to optimise.\n",
    "        gaussian_process: GaussianProcessRegressor object.\n",
    "            Gaussian process trained on previously evaluated hyperparameters.\n",
    "        evaluated_loss: array-like, shape = [n_obs,]\n",
    "            Numpy array that contains the values off the loss function for the previously\n",
    "            evaluated hyperparameters.\n",
    "        greater_is_better: Boolean.\n",
    "            Boolean flag that indicates whether the loss function is to be maximised or minimised.\n",
    "        bounds: Tuple.\n",
    "            Bounds for the L-BFGS optimiser.\n",
    "        n_restarts: integer.\n",
    "            Number of times to run the minimiser with different starting points.\n",
    "\n",
    "    \"\"\"\n",
    "    best_x = None\n",
    "    best_acquisition_value = 1\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    for starting_point in np.random.uniform(bounds[:, 0], bounds[:, 1], size=(n_restarts, n_params)):\n",
    "\n",
    "        res = minimize(fun=acquisition_func,\n",
    "                       x0=starting_point.reshape(1, -1),\n",
    "                       bounds=bounds,\n",
    "                       method='L-BFGS-B',\n",
    "                       args=(gaussian_process, evaluated_loss, greater_is_better, n_params))\n",
    "\n",
    "        if res.fun < best_acquisition_value:\n",
    "            best_acquisition_value = res.fun\n",
    "            best_x = res.x\n",
    "\n",
    "    return best_x\n",
    "\n",
    "\n",
    "def bayesian_optimisation(n_iters, sample_loss, bounds, x0=None, n_pre_samples=5,\n",
    "                          gp_params=None, random_search=False, alpha=1e-5, epsilon=1e-7):\n",
    "    \"\"\" bayesian_optimisation\n",
    "\n",
    "    Uses Gaussian Processes to optimise the loss function `sample_loss`.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        n_iters: integer.\n",
    "            Number of iterations to run the search algorithm.\n",
    "        sample_loss: function.\n",
    "            Function to be optimised.\n",
    "        bounds: array-like, shape = [n_params, 2].\n",
    "            Lower and upper bounds on the parameters of the function `sample_loss`.\n",
    "        x0: array-like, shape = [n_pre_samples, n_params].\n",
    "            Array of initial points to sample the loss function for. If None, randomly\n",
    "            samples from the loss function.\n",
    "        n_pre_samples: integer.\n",
    "            If x0 is None, samples `n_pre_samples` initial points from the loss function.\n",
    "        gp_params: dictionary.\n",
    "            Dictionary of parameters to pass on to the underlying Gaussian Process.\n",
    "        random_search: integer.\n",
    "            Flag that indicates whether to perform random search or L-BFGS-B optimisation\n",
    "            over the acquisition function.\n",
    "        alpha: double.\n",
    "            Variance of the error term of the GP.\n",
    "        epsilon: double.\n",
    "            Precision tolerance for floats.\n",
    "    \"\"\"\n",
    "\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "\n",
    "    n_params = bounds.shape[0]\n",
    "\n",
    "    if x0 is None:\n",
    "        for params in np.random.uniform(bounds[:, 0], bounds[:, 1], (n_pre_samples, bounds.shape[0])):\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "    else:\n",
    "        for params in x0:\n",
    "            x_list.append(params)\n",
    "            y_list.append(sample_loss(params))\n",
    "\n",
    "    xp = np.array(x_list)\n",
    "    yp = np.array(y_list)\n",
    "\n",
    "    # Create the GP\n",
    "    if gp_params is not None:\n",
    "        model = gp.GaussianProcessRegressor(**gp_params)\n",
    "    else:\n",
    "        kernel = gp.kernels.Matern()\n",
    "        model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                            alpha=alpha,\n",
    "                                            n_restarts_optimizer=10,\n",
    "                                            normalize_y=True)\n",
    "\n",
    "    for n in range(n_iters):\n",
    "\n",
    "        model.fit(xp, yp)\n",
    "\n",
    "        # Sample next hyperparameter\n",
    "        if random_search:\n",
    "            x_random = np.random.uniform(bounds[:, 0], bounds[:, 1], size=(random_search, n_params))\n",
    "            ei = -1 * expected_improvement(x_random, model, yp, greater_is_better=True, n_params=n_params)\n",
    "            next_sample = x_random[np.argmax(ei), :]\n",
    "        else:\n",
    "            next_sample = sample_next_hyperparameter(expected_improvement, model, yp, greater_is_better=True, bounds=bounds, n_restarts=100)\n",
    "\n",
    "        # Duplicates will break the GP. In case of a duplicate, we will randomly sample a next query point.\n",
    "        if np.any(np.abs(next_sample - xp) <= epsilon):\n",
    "            next_sample = np.random.uniform(bounds[:, 0], bounds[:, 1], bounds.shape[0])\n",
    "\n",
    "        # Sample loss for new set of parameters\n",
    "        cv_score = sample_loss(next_sample)\n",
    "\n",
    "        # Update lists\n",
    "        x_list.append(next_sample)\n",
    "        y_list.append(cv_score)\n",
    "\n",
    "        # Update xp and yp\n",
    "        xp = np.array(x_list)\n",
    "        yp = np.array(y_list)\n",
    "\n",
    "    return xp, yp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" plotters.py\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_iteration(first_param_grid, sampled_params, sampled_loss, first_iter=0, alpha=1e-5,\n",
    "                   greater_is_better=True, true_y=None, second_param_grid=None,\n",
    "                   param_dims_to_plot=[0, 1], filepath=None, optimum=None):\n",
    "    \"\"\" plot_iteration\n",
    "\n",
    "    Plots a line plot (1D) or heatmap (2D) of the estimated loss function and expected\n",
    "    improvement acquisition function for each iteration of the Bayesian search algorithm.\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "        first_param_grid: array-like, shape = [n, 1]\n",
    "            Array containing the grid of points to plot for the first parameter.\n",
    "        sampled_params: array-like, shape = [n_points, n_params]\n",
    "            Points for which the value of the loss function is computed.\n",
    "        sampled_loss: function.\n",
    "            Values of the loss function for the parameters in `sampled_params`.\n",
    "        first_iter: int.\n",
    "            Only plot iterations after the `first_iter`-th iteration.\n",
    "        alpha: float\n",
    "            Variance of the error term in the GP model.\n",
    "        greater_is_better: boolean\n",
    "            Boolean indicating whether we want to maximise or minimise the loss function.\n",
    "        true_y: array-like, shape = [n, 1] or None\n",
    "            Array containing the true value of the loss function. If None, the real loss\n",
    "            is not plotted. (1-dimensional case)\n",
    "        second_param_grid: array-like, shape = [n, 1]\n",
    "            Array containing the grid of points to plot for the second parameter, in case\n",
    "            of a heatmap.\n",
    "        param_dims_to_plot: list of length 2\n",
    "            List containing the indices of `sampled_params` that contain the first and\n",
    "            second parameter.\n",
    "        optimum: array-like [1, n_params].\n",
    "            Maximum value of the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the GP\n",
    "    kernel = gp.kernels.Matern()\n",
    "    model = gp.GaussianProcessRegressor(kernel=kernel,\n",
    "                                        alpha=alpha,\n",
    "                                        n_restarts_optimizer=10,\n",
    "                                        normalize_y=True)\n",
    "\n",
    "    # Don't show the last iteration (next_sample is not available then)\n",
    "    for i in range(first_iter, sampled_params.shape[0] - 1):\n",
    "        model.fit(X=sampled_params[:(i + 1), :], y=sampled_loss[:(i + 1)])\n",
    "\n",
    "        if second_param_grid is None:\n",
    "            # 1-dimensional case: line plot\n",
    "            mu, std = model.predict(first_param_grid[:, np.newaxis], return_std=True)\n",
    "            ei = -1 * expected_improvement(first_param_grid, model, sampled_loss[:(i + 1)],\n",
    "                                           greater_is_better=greater_is_better, n_params=1)\n",
    "\n",
    "            fig, ax1, ax2 = _plot_loss_1d(first_param_grid, sampled_params[:(i + 1), :], sampled_loss[:(i + 1)], mu, std, ei, sampled_params[i + 1, :], yerr=alpha, true_y=true_y)\n",
    "        else:\n",
    "            # Transform grids into vectors for EI evaluation\n",
    "            param_grid = np.array([[first_param, second_param] for first_param in first_param_grid for second_param in second_param_grid])\n",
    "\n",
    "            mu, std = model.predict(param_grid, return_std=True)\n",
    "            ei = -1 * expected_improvement(param_grid, model, sampled_loss[:(i + 1)],\n",
    "                                           greater_is_better=greater_is_better, n_params=2)\n",
    "\n",
    "            fig, ax1, ax2 = _plot_loss_2d(first_param_grid, second_param_grid, sampled_params[:(i+1), param_dims_to_plot], sampled_loss, mu, ei, sampled_params[i + 1, param_dims_to_plot], optimum)\n",
    "\n",
    "        if filepath is not None:\n",
    "            plt.savefig('%s/bo_iteration_%d.png' % (filepath, i), bbox_inches='tight')\n",
    "\n",
    "\n",
    "def _plot_loss_1d(x_grid, x_eval, y_eval, mu, std, ei, next_sample, yerr=0.0, true_y=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8,8), sharex=True)\n",
    "\n",
    "    # Loss function plot\n",
    "    ax1.plot(x_grid, mu, label = \"GP mean\")\n",
    "    ax1.fill_between(x_grid, mu - std, mu + std, alpha=0.5)\n",
    "    ax1.errorbar(x_eval, y_eval, yerr, fmt='ok', zorder=3, label=\"Observed values\")\n",
    "    ax1.set_ylabel(\"Function value f(x)\")\n",
    "    ax1.set_xlabel(\"x\")\n",
    "\n",
    "    if true_y is not None:\n",
    "        ax1.plot(x_grid, true_y, '--', label=\"True function\")\n",
    "\n",
    "    # Acquisition function plot\n",
    "    ax2.plot(x_grid, ei, 'r', label=\"Expected improvement\")\n",
    "    ax2.set_ylabel(\"Expected improvement (EI)\")\n",
    "    ax2.set_title(\"Next sample point is C = %.3f\" % next_sample)\n",
    "    ax2.axvline(next_sample)\n",
    "\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "\n",
    "def _plot_loss_2d(first_param_grid, second_param_grid, sampled_params, sampled_loss, mu, ei, next_sample, optimum=None):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8,8), sharex=True, sharey=True)\n",
    "\n",
    "    X, Y = np.meshgrid(first_param_grid, second_param_grid, indexing='ij')\n",
    "\n",
    "    # EI contour plot\n",
    "    cp = ax1.contourf(X, Y, ei.reshape(X.shape))\n",
    "    plt.colorbar(cp, ax=ax1)\n",
    "    ax1.set_title(\"Expected Improvement. Next sample will be (%.2f, %.2f)\" % (next_sample[0], next_sample[1]))\n",
    "    ax1.autoscale(False)\n",
    "    ax1.axvline(next_sample[0], color='k')\n",
    "    ax1.axhline(next_sample[1], color='k')\n",
    "    ax1.scatter(next_sample[0], next_sample[1])\n",
    "    ax1.set_xlabel(\"C\")\n",
    "    ax1.set_ylabel(\"gamma\")\n",
    "\n",
    "    # Loss contour plot\n",
    "    cp2 = ax2.contourf(X, Y, mu.reshape(X.shape))\n",
    "    plt.colorbar(cp2, ax=ax2)\n",
    "    ax2.autoscale(False)\n",
    "    ax2.scatter(sampled_params[:, 0], sampled_params[:, 1], zorder=1)\n",
    "    ax2.axvline(next_sample[0], color='k')\n",
    "    ax2.axhline(next_sample[1], color='k')\n",
    "    ax2.scatter(next_sample[0], next_sample[1])\n",
    "    ax2.set_title(\"Mean estimate of loss surface for iteration %d\" % (sampled_params.shape[0]))\n",
    "    ax2.set_xlabel(\"C\")\n",
    "    ax2.set_ylabel(\"gamma\")\n",
    "\n",
    "    if optimum is not None:\n",
    "        ax2.scatter(optimum[0], optimum[1], marker='*', c='gold', s=150)\n",
    "\n",
    "    return fig, ax1, ax2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this algorithm behaves, we'll use it on a classification task. Luckily for us, scikit-learn provides helper functions like `make_classification()`, to build dummy data sets that can be used to test classifiers.\n",
    "\n",
    "We'll optimize the penalization parameter $C$, and kernel parameter $\\gamma$, of a support vector machine, with RBF kernel. The loss function we will use is the cross-validated area under the curve (AUC), based on three folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import svm, metrics,preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve, auc\n",
    "import sklearn.gaussian_process as gp\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path='/home/sultan/EEG/Source_Level_Analysis/50tr10ms_all_clear_erp.csv'\n",
    "path='/home/sultan/EEG/Source_Level_Analysis/25sam_10ms_noise_all_erp.csv'\n",
    "# path='/home/sultan/EEG/Source_Level_Analysis/50Tr10msnoise_all_erp.csv'\n",
    "# path=\"/home/sultan/EEG/Source_Level_Analysis/75sam_10ms_noise_all_erp.csv\"\n",
    "# path=\"/home/sultan/EEG/Source_Level_Analysis/100sam_10ms_noise_all_erp.csv\"\n",
    "dataset =pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc=dataset.iloc[:,2:].values\n",
    "y=dataset.iloc[:,1].values\n",
    "X=preprocessing.scale(Xc)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.20, random_state=40) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=X_train;target=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, target = make_classification(n_samples=2500,\n",
    "#                                    n_features=45,\n",
    "#                                    n_informative=15,\n",
    "#                                    n_redundant=5)\n",
    "def sample_loss(params):\n",
    "    return cross_val_score(SVC(C=10 ** params[0], gamma=10 ** params[1], random_state=12345),\n",
    "                           X=data, y=target, scoring='roc_auc', cv=5).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a relatively simple problem, we can actually compute the loss surface as a function of $C$ and $\\gamma$. This way, we can get an accurate estimate of where the true optimum of the loss surface is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.79166667, -3.21052632])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lambdas = np.linspace(1, 10, 5)\n",
    "# gammas = np.linspace(0.1, 1, 5)\n",
    "lambdas = np.linspace(1, -4, 25)\n",
    "gammas = np.linspace(1, -4, 20)\n",
    "# We need the cartesian combination of these two vectors\n",
    "param_grid = np.array([[C, gamma] for gamma in gammas for C in lambdas])\n",
    "\n",
    "real_loss = [sample_loss(params) for params in param_grid]\n",
    "\n",
    "# The maximum is at:\n",
    "param_grid[np.array(real_loss).argmax(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWIAAAEWCAYAAABc752tAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAF5ZJREFUeJzt3c1uJNd5xvHntWRJiWK4TY7ijREnPYATIDuS2gRZGHYPkAvo8WyzEecORvEqS4O8gABDXQEtbgM4GI4RrzXDTWIgMMCGBQiGDQ85LWdjKbbfLOoUeVisj272x6mq/v+ABru7vtnkw8O3zqkydxcAIJ2vpN4BANh0BDEAJEYQA0BiBDEAJEYQA0BiBDHQUWY2SL0PWA6CGOggMxtJ+sEatvPxqreBHgaxmb02s/PCY2BmQzN7HeYZmNl58fmM6z/vc0tk3u/HHda/Y2YvzezlurZdt80lrf/qZ2uNDtz9aA3bOTazgzVsZ6P1LoiDXXe/Hz2m7j6R9Depd6yOme2n3odZLbCvB5K+7+67y9yfdW8zPv5V/myZ2cjMPg4NjGG07eMlb2PfzMbh+Y6ZPZEkdz+RNFrWtlCur0Fcyt2nqfehwePUOzCHu+7rMMHnsIpt3jj+VRxTCMOhuz+UdCkp/0/soaSTJW3jqaSBux+5+4m7n0oaStqOZnthZjvL2B7KbUwQz/pvr5k9CeWHZ3kJwswOwnsfS9qqWG4/KoUcRO+9DI9RtB8vzexpYd6nkvJ/oZuWP4+2m5de8vUehBbUMBzDSzN7VvX9iPbjac1xFffh1r7OuNyBpGH8XsNnUbaOG8c1w3He2OYM378bn0u0H1efbfH4S9Y58+dec+xDSY/z8kP4z+4sTN4LrfCFhKA/D63e2Jmk+Ht5LlrFq+XuvXpIei3pZfQ4CO8PlP3Q1T3fkfRxeD6U9FTZD+DLwvoHhW3uFOYZhveeRe+9DNsaSHJlLR1Jeh3Nc15YZ9Xy8XznhfU+Ce8fSBrXfJ8G8bGE9Y9Kvh+39qG4ryXrrl2u+P0r7FPttovH1XScxW3O8P278bmUfbYln9Us+135uVfs80H+WZb9jJe891RZeOY/78N8W3W/KzP+To3z9fJYzaOvLeLvu/tueHw4x3KPlLWenin7wR5KehCe5y4rlruq2XnWWnlUWO5Y12e585q1JF1a+cm/uuWrTN39MDx/Jumj0GIbVsx/6df/Uj9VdqyL7sMiy82yjuJxzXKcsyr7XMo+27vsd9X6q4xVXX648TNoZmN3f+zuDyQ9C9+HYd2+hpb6i5rtF/X2BHUb9DWIF/Ejd3+QP+ZYrlgjLP7gxjW3sjAvqlu+ytV6Pav17Uq6UNYqm8XFEvZhkeUa11E8rjseZ5Wqz2We+u+in7tCPXZaE6Q3ymMelRbC92MkqekPxkAVx1VSNtqqmhfLQRDfdKzoJEz4hXim7ORI3oG+rNVVttzH+XthubGk04btT/N6ZdXyUQu2bn9kZkN3n4QW8ouK1uIwapV9WLJ/dccQ72vRXY59pnWUHNd3ZzjOK7N+/yJln61UffzLOPbHKpwgC7XwvKdGU6APZmi55yflbjCzUQjzG+tTVsLBihDEEc9Ohnxs131O98IP5SScjDlQScsgLJefhDmXlP8wPwuvn0v6cMZfjryuXbf8JOzfgapbPqNwHOeSJhXbnir7t/5c0olfnwzKj6tuH672teT7cZdjn3UdN45L0ndmOM6iWb5/+X7c+mzDpNLjX/TYQ/ieK/tsDizrtvZSWR087zc8afiDc7W9/GRkyXFNJT0MJZ2821pZCEvS+5qvjIE5mTsXht9E4ZfzpbvfT70vmI+ZjZXVgA9Lpg2VtYjPwuun7r5Qt0gze+nr7fe9cZbSIm7qigNgeUJN+FHF5J3CfzYLDVEOob+0wSMot3AQh7rVeAn7AmB2H5Q1gLzQJ7ii1DCT8F/Tg7KW96aJyjelI0otG38wjqdbNkJxXLVMbOEgDnWrhTuXY708G/ZNWaKj3P1szq6Zd9nGdNGyRh/kJ03zP2pWGGWY9zIJfwTvR/X7H4b3BsVlijhZBwD1Hun6JP1Et0cZPtB1Y/Rc2QnlsaRPJMndD4snwoveXN6+lgvN8n1JekNv7r775jdWvUkAPfC7P/z2lbu/t8g6/vG77/j08k+N8/38v/7v55J+H711FPVSGehml8Fiv/gLXfftHoTp96Wr1vOoqbyz8iAOB3MkSV//6l/6P9x7uOpNAuiBn/z63z5ddB3Tyz/px//enOV//1e/+r27791xMye67mt+X1mreFvShbufhdryuFi/j1GaAIB6U91s8d4YgRr6iR/nIyKVlSkudF2umCrri11pGb0mxpL2ZjkzCAAddKzrUYhDhZGS+UCZEMB7oQ48CC3fk2iZgUK9uMoyek2cuPs3fD13CwCAtYoGx4yUXQMkP/H2PJp+GRqlT8N7E2XD4MeStuvKEtIaasQA0HVlDc14tGFZ0EbLNF7EnxoxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYtwqCQAahHvPTSUNy26bZGZPlN21eSuf3rRMjBYxANQId2mWu5/Gr6PpozD9RNJ9Mxs2LVNEEANAvUfKWrZS1uodFaY/CO9L0nmY3rTMDZQmAKDeQNJl9Hq7MP1C0lY07/YMy9xAEAPordd//HOdfL7bPKN+dc/MXkRvHDXVdSMnkh6H5/eVtYoHc+wmQQwAkl65+17FtKlutngv4onuPjGz41AHniorRWzXLVNEjRgA6h1LGobnQ0n5CbhB+Lojac/dzyQNwkm70mWqEMQAUCMEbN47Ypq/lvQ8mn4Zuqs9bVimFKUJAGhQVi92993o+cksy1ShRQwAiRHEAJAYQQwAiRHEAJAYQQwAiRHEAJAYQQwAiRHEAJDYwgM65rn4MQDgtoVaxPNe/BgAcNuipYm5Ln4MALht0dJE48WPzWxf0r4kvfOVv1hwcwDQPys/WefuR+6+5+57b33lz1a9OQDonEWDuPaCyQCAZosG8VwXPwYA3LZQEM978WMAwG0L9yOm7zAALIaRdQCQGEEMAA3MbGxmo9Add67pZvakaf0EMQDUaBpBHF5PwvRJPD2cP3vQtA2CGADqzTKC+CB8Hd6l0wJBDAD1akcQh+CdmNnreD4z28lb0U0W7jUBAG31uz+8o5/+5juzzHrPzF5Er49m7RFmZgNJ55I+kPSRmZ25+0TXg90aEcQAIL1y972KaU0jiPeVBffUzKaSxmZ2OmtrWCKIAaDJsaQ8pK9GEJvZwN2nkhR9PTWzoaRh+LolaSuUKSprxwQxANRw9zMz2ysZQfxc0q67H4YuahNJW3FJI3RnGzRtgyAGgAZl9WJ3342eH9Ys11hrptcEACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAYgQxACRGEANAAzMbm9ko3Ay0OG3HzNzMzsPjadMyRQQxANQwsx1JcvfT+HVky93N3e9LeijpIMwzCctMSpa5gSAGgHqPJE3D84mkUTwxD+hgz90n4flB+Dp097O6Dby5jL0EgDb68ss39cvP3ptl1ntm9iJ6feTuR+H5QNJlNG27bAVmNpL0Y0ly9zMzm5jZa0kfNG2cIAY65Mu/+1bqXVifX691a6/cfW/BdTyIyhcDSefKQvgjMzuLWsq3LCWIzezA3T9cxrqAu9iogMK6TSVthecDSRcV88V14H1lreqpmU0ljSUdVm1g4SAOZwTHkgjixAgjYCWOJeWt5aGkq1avu0/D82FxoXyau5+WTY8tHMTufmRmDxddz7oQVgDmEeq9e6EGPI1OvD2XtBvNOomWOTSzJ+G9rajeXGrlNeLQYt6XpLffHhCEADqnLEjdfTd6PpH0uDC9shRRtPLua+5+5O577r731lffXfXmAKBzGlvEFaNCLt39ZAX7A2AOnw/fTr0Lq/OfqXdgfRqDuKm2AWyqXocg1moZvSbGkvbMbJ/QxioQeOi7ZfSaOJFEmaJDCDagXRhZtwQEG4BFrDWI//i2EVoAUECLGGi5//22pd4FrBhBjM4jqNB1BHGPEVBAN2x0EBNUANpgzSfrCD8AKOJWSQCQ2EaXJrAZvvj2l6l3AahFEHcIgQL0E0F8R4QigGXZ2CAmSAG0ReeCmAAF0DdrDWJ/ywlSAJ0TLvc7lTQsXu7XzHYkvdT1PetO3f1xdFON+013uaf7GgDUCEErdz+NX0e23N3c/b6kh5IOwo1GT0NoD8PrSgQxANR7pKw1LGWt3huhmgd0sBduJDqM5stfV+pcjRgAVuCemb2IXh9FJYiBpMto2nbZCkKr98fSrVvM7Ug6rts4QQygt+xL09ufvjXLrK/cfW/BzT0otI7zMsaZu5/VLUhpAgDqTSVthecDSRcV8xVrx5I0ajpRJxHEANDkWNc13qGk/KTdIJ/BzG7VgMMNlQ/Dc07WAcBd5WWFEKbTqMzwvDBr3n0tn/fAzM7N7HXTNqgRA0CDYt/h8N5u9Hwi6XH0+lTSN2ZdPy1iAEiMIAaAxAhiAEiMIAaAxAhiAEiMIAaAxAhiAEiMIAaAxAhiAEiMIAaAxAhiAEiMIAaAxAhiAEhs4auvzXOnUgDAbQu1iOe9UykA4LZFSxNz3akUAHDbQqWJWe5UGkoX+5L0xvagOBkANt5STtbV3anU3Y/cfc/d99742rvL2BwA9Epjizg6GRe7dPeT6PVMdyoFgC4ys7GyuzkPy26bFBqjQ0lb+fSy96o0tohDi7b4uArhee5UCgBdEwI1vw/d1euCH4ZcHETTy94rtYxeEzPfqRQAOuiRstawlHVKuNHgDK3lTyTJ3Q/d/azsvboNLHqybq47lQJAS90zsxfR66OonDCQdBlN2y4s+7501VIehQpB2XuVFh7QAQBt9cYX0tc+9VlmfeXuewts6iK0hEehNXzrvcJ5tRsY4gwA9aaStsLzgaSLwvQLZSWLfN73K96rRBADQL1jXQ9WG0rKT9rlAyNOoukDZbXhsvcqEcQAUCM/0RY6J0yjE2/Pw/SJpGkoSWy7+0nZe3XboEYM9Nhff+u3qXfhzj5NvQORsn7A7r5bMv2k7r0qBDHQI10O3k1GEAMdRvD2A0EMdAzh2z8EMdARBHB/EcRAixG+m4EgBlqIAN4sBDHQEoTv5iKIgYQIX0gEMbBWBC/KEMTAihG+aEIQAytA+GIeBDGwJF0P3+998xepd+GGn6XegTUiiIEFtD182xauKEcQA3fQ5gAmfLuHIAZmRPhiVQhioEFbA5jw7Q+CGKjQxgAmfPuJIAYKCGCsG0EMRNoWwgRwO4R7z00lDctum2RmO8puFrqVT29aJsbNQwFlAdymEP7eN39BCLdECFm5+2n8uuCH4QahAzPbmXGZKwQxNl6bAliiFdxCj5S1bCVpImkUTwwt308kyd0Pw12ea5cpIoix0QhhBPfM7EX02I+mDSRdRq+3C8u+L2k7tISfzLjMDdSIsbEI4f574wvX1ydfzDLrK3ffW2BTF+5+Zmaj0EKeCy1ibCRCGHOYStoKzweSLgrTL5SVH/J5359hmRsIYmwcQhhzOlbWI0Lha34CbhDeO4mmD5TVi0uXqUIQY6MQwphXOPkmMxtJmuavJT0P0yeSpqEkse3uJzXLlKJGjI3RphAmgLulrB+wu++WTD+pW6YKQYyNQAiXG3/9ZepdqPSvqXdgjQhi9FqbAlhKF8JtDlwQxOgxQpgA7oqFT9aZ2Tj0nXu6jB0CgE2zUBCHM4IPwnjqYdN4agDrdfL5bvNMSG6h0kQI4Lx/3FZZF40wVHBfkt7YHhQnA8DGW0ZpYhDGV/+obLq7H7n7nrvvvfG1dxfdHAD0zsJB7O5Tdz+U9NjMho0LAABuaCxNFK5ClLt095Pomptnks4kjSUdLncXASzi5PNdek+0XGMQN4wOGSkLYOl6jDUAYA6LliaOlPWW2Fc2nvqkaQEA60fviXZbtNfEVFkYA63StsEcQB2uvgasQRuuL0GruL0IYmCDEMbtRBADQGIEMbAGP/3Nd1LvwhVaxe1DEKOXfvnZe6l34Za2hTGB3B4EMXqrjWHcNoTxbKKrTJYNcJOZHYSvt6aHS0DUIoiBNWpTqzhHGNeLRhCfxq8L9s3sXNd3c86XHUl60LQNghi91sZWcVvDmECu9EjSNDyfKBtRXPSBu9/Pw3peBDGAK4RxqYGky+j1dsk8W6F0cVWGMLOdWYOZIEbv0SqeD63j+YXL/Z5K2g7lCEnamnV57lkHJPLT33ynFSPuqvThqm32+y/11v98Nsus98zsRfT6KLrg2VTXoTqQdHFjG9kJustwrZ0LhbsVzVOmIIixEX752Xtcf+IO8pZx1wN5Bq/cfa9i2rGkfNpQ4a5EZjYI19uZSMpDfDtMH4brs28pK1vslN3BKEdpAkiozSWK2CaXKvIADSWHaRSoz8P0U0kjMxtLunD3M3c/ia5G2XiPOFrE2BhtbRW3vUSR60Op4q7Krsvu7rvR89JLAIflGq9QSYsYaIEutYw3uXW8KgQxNkobe1DkuhLGEoG8bJQmgBbpSpkiF4fxppYtloEWMTZOm1vFUhbGXWod52gl3x1BjI3U9jCWulWqiBHI86M0AbRY10oVMcoWs6NFjI3VhVax1N1SRYxWcj2CGOiIroexRCBXIYix0brSKs71oXUsEchFBDHQQX0IY4lAzhHE2HhdaxXn+hLGEoFMEAPqdhgTyN1HEANSKy8GNI8+BvImoR9xAl3/pUd75WHc1b7Hm4ogXgOCF+vW5YEgm4ggXhHCF6nROu4OgnhJCF60FYHcfpysAzZE307o9QlBvAS0htElBHL7LC2IzezJstYFYPUI5NmZ2djMRma23zDfk3mXkZYUxOHupg+Wsa6uoTWMriOQ65nZjnR1t+ar1yXzXeVgmGcSlplULZOjNAFAEoFc45GkaXg+kTSacbmD8HXo7md1My4cxGa2k/+l2DS0htFHBPItA0mX0evt4gzFHAzBOzGz14VlSy2j+9pW3cRQH8lrJF98+s//8t9L2GYrfJp9uSfpVdIdWQ2OqztWckw/W/YK5/e3i67gd3/47X/85Nf/dm+GWd8xsxfR6yN3P5pjUzdy0MwGks4lfSDpIzM7c/dJ1cKNQVxRaL5095NZWsPhYI7Cul64+17TNrukj8ckcVxd0sdjkrLjWnQd7v5PS9iVqa6DdiDpIp5YkYP7ysJ8amZTSWNJh1UbaAzihr8KQzMbhp3cCjtUWwsBgI45lpT/oRtKyk/aDdx9qpIclKQwTe5+GqZXWqhG7O4n7n4SXg4WWRcAtFHeuAy9IqZRY/N5mH4rB9390MyehC5s+01lDnP3Fe1+ycZm2KGu6eMxSRxXl/TxmKT+HleZtQYxAOA2+hEDQGLJg5ih0UjFzA6a50IKm/bZJA3ivg2NjsaWP029L8tkZvvh0ZtfjtAtc5x6PxY1z/UMuqIvn808kreI+yL/oxL6Ew6bxpZ3RTiu03DSZBhed144nsoO9l0w6zUQuqYPn828kgVx34ZGu/upuz8OL7d61J96qOux9ZPwGu1w12sgoGVS3qGjdmh0F4VhjfuSfpR6X5al0H1oR1nndrRD4zUQ0A0rC+JFh0Z3URhJc2hmz5rGlndN+Lf3rCst/bqfv7XvDNBgZUHcx6HRTX9cpKtROGdqGFveJjOG1sjdP1zXPi1qQwYC1F4DAd2RpDSR/4KHAOjM0OiGX+6RsgCWsmP6ZPV7tByNwy+zEU6H4fmoD//NmNlY0l7HR2+VXgOh63ry2cyFkXVLEurDPwgv73ep9Vgn9JL4WFktckvSwz4EcV+ExsxE2cXHNyK0+oggBoDE6EcMAIkRxACQGEEMAIkRxACQGEEMAImlHOKMDRcNCZ8q6x43lTRg9Bs2DUGMJMLIygNJH+Q3WQyX2XyWdMeABAhipPJU0uM8hINPGCyCTUSNGGsXXZfjxkWRKElgUxHESGFP19flADYeQYxUbl0prC93mADmRRAjhVNJ78dvhKu60UrGRuKiP0giXOpwqOt7k50WTtwBG4MgBoDEKE0AQGIEMQAkRhADQGIEMQAkRhADQGIEMQAkRhADQGL/D9NN9eXiISOTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import rc\n",
    "rc('text', usetex=True)\n",
    "C, G = np.meshgrid(lambdas, gammas)\n",
    "plt.figure()\n",
    "cp = plt.contourf(C, G, np.array(real_loss).reshape(C.shape))\n",
    "plt.colorbar(cp)\n",
    "plt.title('Filled contours plot of loss function $\\mathcal{L}$($\\gamma$, $C$)')\n",
    "plt.xlabel('$C$')\n",
    "# plt.ylabel('$\\gamma')\n",
    "# plt.savefig('/Users/thomashuijskens/Personal/gp-optimisation/figures/real_loss_contour.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the underlying GP, we'll assume a [Matern](http://scikit-learn.org/stable/modules/gaussian_process.html#matern-kernel) kernel as the covariance function. Although we skim over the selection of the kernel here, in general the behaviour of the algorithm is dependent on the choice of the kernel. Using a Matern kernel, with the default parameters, means we implicitly assume the loss $f$ is at least once differentiable. [There are a number of kernels available](http://scikit-learn.org/stable/modules/gaussian_process.html#kernels-for-gaussian-processes) in scikit-learn, and each kernel implies a different assumption on the behaviour of the loss $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounds = np.array([[-4,1], [-4, 1]])\n",
    "\n",
    "# xp, yp = bayesian_optimisation(n_iters=30, \n",
    "#                                sample_loss=sample_loss, \n",
    "#                                bounds=bounds,\n",
    "#                                n_pre_samples=3,\n",
    "#                                random_search=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The animation below shows the sequence of points selected, if we run the Bayesian optimization algorithm in this setting. The star shows the value of $C$ and $\\gamma$ that result in the largest value of cross-validated AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rc('text', usetex=False)\n",
    "# plot_iteration(lambdas, xp, yp, first_iter=3, second_param_grid=gammas, optimum=[1, -3.21])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.165950018614822"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**0.79"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
